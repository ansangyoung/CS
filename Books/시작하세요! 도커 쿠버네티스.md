# 시작하세요! 도커/쿠버네티스  

### 용찬호 지음  

## 01 도커란?  
도커는 리눅스 컨테이너에 여러 기능을 추가함으로써 애플리케이션을 컨테이너로서 좀더 쉽게 사용할 수 있게 만들어진 오픈소스 프로젝트이다.  
도커는 Go 언어로 작성돼 있으며, 기존에 쓰이던 가상화 방법인 가상 머신과는 달리 도커 컨테이너는 성능의 손실이 거의 없어서 차세대 클라우드 인프라 솔루션으로서 많은 개발자들에게 주목받고 있다.  


## 02 도커 엔진  
### 2.1 도커 이미지와 컨테이너  
### 2.1.2 도커 컨테이너  
도커 이미지는 기본적인 리눅스 운영체제부터 각종 애플리케이션, 빅데이터 분석 도구까지 갖가지 종류가 있다.  
이러한 이미지로 컨테이너를 생성하면 해당 이미지의 목적에 맞는 파일이 들어있는 파일시스템과 격리된  
시스템 자원 및 네트워크를 사용할 수 있는 독립된 공간이 생성되고, 이것이 바로 도커 컨테이너가 된다.  
도커 이미지와 컨테이너는 1:N 관계이다.  


### 2.5 도커 데몬  
#### 2.5.1 도커의 구조  
a. 도커 명령어의 위치 확인  
which docker  
-> /usr/bin/docker  
-> 참고: 리눅스의 which 명령어는, 명령어의 파일이 위치한 경로를 출력한다.  
-> 의미: 도커 명령어는 /usr/bin/docker에 위치한 파일을 통해 사용되고 있음. 즉, 컨테이너나 이미지를 다루는 도커 명령어는 /usr/bin/docker에서 실행한다.  

b. 실행 중인 도커 프로세스를 확인  
ps aux | grep docker  
-> USER  PID    %CPU  %MEM  VSZ     RSS    TT  STAT  START  TIME  COMMAND  
   root  20907  0.0   6.7   594500  68872  ?   Ssl   16:28  0:05  /usr/bin/dockerd -H fd://
-> 참고: ps aux 명령어는, 실행 중인 프로세스의 목록을 출력한다. (CPU, MEM사용률, 프로세스 상태 코드 등 확인 가능.)  
-> /usr/bin/dockerd 파일로 도커 엔진의 프로세스를 실행한다. (docker가 아닌 dockerd(도커 데몬)으로 실행하는 이유는, docker 명령어는 클라이언트이기 때문이다.)  

c. 도커의 구조  (클라이언트 + 서버)  
도커 엔진은 외부에서 API 입력을 받아 도커 엔진의 기능을 수행하는데, 도커 프로세스가 실행되어 서버로서 입력을 받을 준비가 된 상태를 "도커 데몬"이라고 한다.  
(데몬: 사용자가 직접적으로 제어하지 않고, 백그라운드에서 돌면서 여러 작업을 하는 프로그램. 일반적으로 프로세스로 실행된다.)  
도커 데몬은 API 입력을 받아 도커 엔진의 기능을 수행하는데, 이 API를 사용할 수 있도록 CLI를 제공하는 것이 "도커 클라이언트"이다.  
실제로 컨테이너를 생성하고 실행하며 이미지를 관리하는 주체는 "도커 서버"로, 도커 데몬 프로세스로서 동작한다.  
-> 즉, 서버로서의 도커 == 도커 데몬 이라고 인지해도 됨.  

d. 도커 데몬을 제어하는 순서  
-> 사용자가 docker로 시작하는 명령어(예를들어, docker version)를 입력하면 도커 클라이언트를 사용하는 것이며, 도커 클라이언트는 입력된 명령어를 로컬에 존재하는 도커 데몬에게 API로서 전달한다.  
-> 도커 데몬은 이 명령어를 파싱하고, 명령어에 해당하는 작업을 수행하며, 수행 결과를 도커 클라이언트에게 반환하고, 사용자에게 결과를 출력한다.  



### 2.5.2 도커 데몬 실행 (2가지 방법) 
도커 데몬은, 일반적으로 service 명령어로 시작, 정지할 수 있다.  
service docker start  
service docker stop  
우분투에서는 도커가 설치되면 자동으로 서비스로 등록되므로, 호스트가 재시작되더라도 자동으로 실행한다.  
그러나, 레드햇 계열의 운영체제는 도커를 설치해도 자동으로 실행되도록 설정되지는 않는다.  
-> 이때, 도커를 자동으로 실행하도록 설정하려면 systemctl enable docker 와 같은 명령어로 docker 서비스를 활성화할 수 있다.  

또한, 서비스를 사용하지 않고 dockerd 입력으로 직접 도커 데몬을 실행할 수 있다.(2.5.1  b.)  
그러나, 실제 운영환경에서는 도커 데몬을 직접 실행하기보다는 service, systemctl 명령어를 통해 리눅스 서비스로서 관리하는 것이 좋다.  
-> 직접 도커 데몬을 실행하면 하나의 터미널을 차지하는 포그라운드 상태로 실행되기 때문이다.  


### 2.5.3 도커 데몬 설정  
#### 2.5.3.1 도커 데몬 제어: -H  
-H 옵션은 도커 데몬의 API를 사용할 수 있는 방법을 추가한다.  
예를들어, -H에 IP 주소와 포트 번호를 입력하면 원격 API인 Docker Remote API로 도커를 제어할 수 있다.  
Remote API는 로컬에 있는 도커 데몬이 아니더라도 제어할 수 있으며, Restful API 형식을 띠고 있으므로 HTTP 요청으로 도커를 제어할 수 있다.  

예를들어, 호스트의 모든 네트워크 인터페이스 카드에 할당된 IP 주소와 2375번 포트로 도커 데몬을 제어함과 동시에, 도커 클라이언트도 사용할 수 있는 방법은 다음과 같다.  
dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375  
(도커 클라이언트는 /var/run/docker.sock에 위치한 유닉스 소켓을 통해 도커 데몬의 API를 호출한다.)  
(도커의 Remote API를 사용하는 포트는 보안이 적용돼 있지 않다면 2375번 포트를, 보안이 적용되어 있다면 2376번 포트를 사용하도록 관례상 설정한다.)  
API에 따라서 사용하는 방법이 도커 명령어와 조금씩 다른 부분도 있으므로, HTTP 도구로 직접 API 요청을 전송하기보다는, 특정 언어로 바인딩된 라이브러리를 사용하는 것이 일반적이다.  


#### 2.5.3.2 도커 데몬에 보안 적용: --tlsverify  
도커를 설치하면 기본적으로 보안 연결이 설정돼 있지 않다. 이는 도커 클라이언트, Remote API를 사용할 때 별도의 보안이 적용되지 않음을 의미한다.  
보안이 적용되어 있지 않으면 Remote API를 위해 바인딩된 IP주소와 포트 번호만 알면 도커를 제어할 수 있다.  
도커 데몬에 TLS보안을 적용하거나 도커 클라이언트와 Remote API 클라이언트가 인증되지 않으면 도커 데몬을 제어할 수 없도록 설정할 수 있다.  
TLS(Transpot Layer Security): 인터넷에서 정보를 암호화해서 송수신하는 프로토콜로 SSL에 기반한 기술이다.  


#### 2.5.3.3 도커 스토리지 드라이버 변경: --storage-driver  
도커는 특정 스토리지 백엔드 기술을 사용해 도커 컨테이너와 이미지를 저장하고 관리한다.  
일부 운영체제는 도커를 설치할 때 기본적으로 사용하도록 설정된 스토리지 드라이버가 있는데, 우분투같은 데비안 계열 운영체제는 overlay2를, 구 버전의 CentOS와 같은 운영체제는 devicemapper를 사용하는 것이 대표적인 예이다.  
이는 docker info 명령어로 확인할 수 있다.  
docker info | grep "Storage Driver"  
-> Storage Driver: overlay2  

스토리지 드라이버의 원리  
도커 스토리지 드라이버는 CoW, RoW를 지원해야 한다.  
스냅숏: 원본 파일은 읽기 전용으로 사용하되, 이 파일이 변경되면 새로운 공간을 할당  
이미지: 읽기 전용 파일로 사용하며, 각 스냅숏에 해당  
컨테이너: 이미지 위에 얇은 컨테이너 레이어를 생성함으로써 컨테이너의 고유한 공간을 생성. 스냅숏을 사용하는 변경점이다. 이전 이미지에서 변경된 사항이 저장되어 있다.  
컨테이너를 이미지로 만들면, 변경된 사항이 스냅샷으로 생성된다.  

CoW (Copy-on-Write)  
스냅숏의 파일에 쓰기 작업을 수행할 때, 스냅숏 공간에 원본 파일을 복사한 뒤, 쓰기 요청을 반영한다.  
이 과정에서 복사하기 위해 파일을 읽는 작업 한 번, 파일을 스냅숏 공간에 쓰고 변경된 사항을 쓰는 작업 한 번, 총 2번의 쓰기 작업이 일어나므로 오버헤드가 발생한다.  
Row (Redirect-on-Write)  
파일을 스냅숏 공간에 복사하는 것이 아니라, 스냅숏에 기록된 원본 파일은 스냅숏 파일로 묶은 뒤, 변경된 사항을 새로운 장소에 할당받아 덮어쓰는 형식이다.  
즉, 스냅숏 파일은 그대로 사용하되, 새로운 블록은 변경 사항으로써 사용하는 것이다.  


#### 2.5.3.4 컨테이너 저장 공간 설정  
컨테이너 내부에서 사용되는 파일시스템의 크기는 도커가 사용하고 있는 스토리지 드라이버에 따라 조금씩 다르다.  
예를들어, AUFS나 overlay2 등의 스토리지 드라이버를 사용하도록 설정되어 있다면, 컨테이너는 호스트와 저장 공간의 크기를 공유한다.  
devicemapper와 같은 드라이버를 사용하고 있다면, 기본적으로 컨테이너는 10GB의 저장 공간을 할당 받는다.  

### 2.5.4 도커 데몬 모니터링  
#### 2.5.4.1 도커 데몬 디버그 모드  
도커 데몬을 디버그모드로 실행하면, Remote API의 입출력뿐만 아니라, 로컬 도커 클라이언트에서 오가는 모든 명령어를 로그로 출력한다.  
디버그모드는 도커 데몬을 실행할 때 -D 옵션을 추가해서 사용할 수 있다.  
그러나 로그에는 원하지 않는 정보까지 너무 많이 출력되며, 도커 데몬을 포그라운드 상태로 실행해야 한다는 단점이 있다.  


#### 2.5.4.2 events, stats, system df 명령어  
events  
도커 자체가 제공하는 기능이며, 도커가 기본으로 제공하는 명령어이다.  
docker events 또는 docker system events로 입력하면, "도커 데몬에 어떤 일이 일어나고 있는지를" 실시간 스트림 로그로 보여준다.  
다음 예는 이미지에 관한 로그만 출력하도록 설정하여 이미지에 관련된 명령어만 출력할 수 있다.  
docker events --filter 'type=image'  

stats  
실행중인 모든 컨테이너의 "자원 사용량"을 스트림으로 출력한다. CPU, 메모리 제한 및 사용량, 네트워크 입출력(I/O), 블록 입출력(하드웨어 입출력) 정보를 출력한다.  
CONTAINER     ID NAME         CPU %  MEM USAGE / LIMIT     MEM %  NET I/O   BLOCK I/O        PIDS  
42974ae6b3de  k8s_jenkins_..  0.06%  663.1MiB  / 7.007GiB  9.24%  0B / 0B   129MB / 3.36MB   39  

system df  
도커에서 사용하고 있는 "이미지, 컨테이너, 로컬 볼륨의 총 개수 및 사용 중인 개수, 크기, 삭제함으로 확보 가능한 공간"을 출력한다.  
TYPE    TOTAL  ACTIVE  SIZE     RECLAIMABLE  
Images  128    21      49.99GB  41.19GB (82%)  


#### 2.5.4.3 CAdvisor  
구글이 만든 컨테이너 모니터링 도구로, 컨테이너로서 간단히 설치할 수 있고 컨테이너별 실시간 자원 사용량 및 도커 모니터링 정보 등을 시각화해서 보여준다.  
오픈소스로서 깃허브에서 소스코드로 사용할 수 있으며, 도커 허브에서 도커 이미지로도 배포되고 있다.  


### 2.5.5 Remote API 라이브러리를 이용한 도커 사용  
-H 옵션을 원격의 도커 데몬을 제어하기 위해 사용하는 것도 좋은 방법이다.  
컨테이너 애플리케이션이 수행해야 할 작업이 많거나 애플리케이션 초기화 등에 복잡한 과정이 포함되어 있다면, 도커를 제어하는 라이브러리를 사용해 이를 좀 더 쉽게 해결할 수 있다.  
즉, 도커를 제어하고 싶을 경우 일일이 Remote API에 대한 요청을 소스코드로 제작할 필요 없이, 이미 Remote API를 래핑해서 사용하기 쉽게 만들어 놓은 라이브러리를 이용할 수 있다.  


## 03 도커 스웜  
### 3.1 도커 스웜을 사용하는 이유  
새로운 서버나 컨테이너가 추가됐을 때 이를 발견하는 작업부터 어떤 서버에 컨테이너를 할당할 것인가에 대한 스커줄러와 로드밸런서 문제, 클러스터 내의 서버가 다운됐을 때 고가용성을 어떻게 보장할지 등이 문제로 남아있다.  
이러한 문제를 해결하는 여러 솔루션을 오픈소스로 활용할 수 있다.  
이 가운데 대표적인 것이 도커에서 공식적으로 제공하는 도커 스웜과 스웜모드이다.  


### 3.2 스웜 클래식과 도커 스웜 모드  
스웜 클래식과 스웜 모드는 여러 대의 도커 서버를 하나의 클러스터로 만들어 컨테이너를 생성하는 여러 기능을 제공한다.  
두가지 종류가 있는데, 첫 번째는 도커 버전 1.6 이후부터 사용할 수 있는 컨테이너로서의 스웜(스웜 클래식)이고, 두 번째는 도커 버전 1.12 이후부터 사용할 수 있는 도커 스웜모드(스웜 모드)이다.  
스웜 클래식은 여러 대의 도커 서버를 하나의 지저머에서 사용하도록 단일 접근점을 제공한다면,  
스웜 모드는 마이크로서비스 아키텍처의 컨테이너를 다루기 위한 클러스터링 기능에 초점을 맞추고 있다.  
일반적으로는 스웜 모드를 더 많이 사용한다.  


### 3.3 스웜모드  
docker info 명령어를 통해 도커 엔진의 스웜 모드 클러스터 정보를 확인할 수 있다.  
docker info | grep Swarm  
-> Swarm: inactive  

### 3.3.1 도커 스웜 모드의 구조  
스웜 모드는 매니저 노드와 워커 모드로 구성돼 있따. 워커 노드는 실제로 컨테이너가 생성되고 관리되는 도커 서버이고, 매니저 노드는 워커 노드를 관리하기 위한 도커 서버이다.  

### 3.3.2 도커 스웜 모드 클러스터 구축  
### 3.3.3 스웜 모드 서비스  
#### 3.3.3.1 스웜 모드 서비스 개념  
스웜 모드에서 제어하는 단위는 컨테이너가 아닌 서비스이다. 컨테이너들은 태스크라고 한다.  


## 04 도커 컴포즈  
### 4.1 도커 컴포즈를 사용하는 이유  
도커 컴포즈는 컨테이너를 이용한 서비스의 개발과 CI를 위해 여러 개의 컨테이너를 하나의 프로젝트로서 다룰 수 있는 작업 환경을 제공한다.  


### 4.4 도커 학습을 마치며: 도커와 컨테이너 생태계  
사실 도커 데몬은 컨테이너가 아니다.  
실제로 컨테이너 프로세스라고 부를 수 있을 만한 것은 dockerd가 아닌 runC이다.  
컨테이너에 1:1로 매칭되는 런타임 역할을 runC가 담당한다.  
그리고 여러 개의 runC 컨테이너 프로세스 및 이미지를 관리하는 주체가 바로 containerd(컨테이너-디)이다.  
도커 엔진(dockerd 프로세스)은 containerd와 통신해 runC를 사용할 수 있도록 하는 엔드유저용 도구에 불과하다.  


## 06 쿠버네티스 시작하기  
### 6.1 쿠버네티스를 시작하기 전에  
쿠버네티스는 대부분의 리소스를 '오브젝트'라고 불리는 형태로 관리한다.  
(스웜모드의 서비스도 컨테이너 리소스의 집합을 정의한 것이기 때문에 일종의 오브젝트라고 볼 수 있다.)  
쿠버네티스는 컨테이너의 집합(pods), 컨테이너의 집합을 관리하는 컨트롤러(Replica Set), 사용자(Service Account), 노드(Node)등을 하나의 오브젝트로 사용할 수 있다.  
1. 쿠버네티스에서 사용할 수 있는 오브젝트 확인   
``kubectl api-resources``  
2. 특정 오브젝트의 간단한 설명 확인  
``kubectl explain pod``  

쿠버네티스도 YAML 파일로 컨테이너 리소스를 생성하거나 삭제할 수 있다.  
(쿠버네티스에서 YAML 파일의 용도는 컨테이너뿐만 아니라, 거의 모든 리소스 오브젝트들에 사용될 수 있다.)  

쿠버네티스 노드의 역할은 크게 마스터와 워커로 나뉘어 있다.  
마스터 노드는 쿠버네티스가 제대로 동작할 수 있게 클러스터를 관리하는 역할을 담당한다.  
워커 노드에는 애플리케이션 컨테이너가 생성된다.  
마스터 노드에는 API 서버(kube-apiserver), 컨트롤러 매니저(kube-controller-manager), 스케줄러(kube-scheduler), DNS(coreDNS) 서버 등이 컨테이너로 실행되며,  
모든 노드에서는 오버레이 네트워크 구성을 위해 프락시(kube-proxy)와 네트워크 플러그인(calico, fiannel 등)이 실행된다.  

쿠버네티스 클러스터 구성을 위해 kubelet이라는 에이전트가 모든 노드에서 실행된다.  
kubelet은 컨테이너의 생성, 삭제뿐만 아니라 마스터와 워커 노드 간의 통신 역할을 함께 담당하는 중요한 에이전트로, 모든 노드에서 기본적으로 실행된다.  


### 6.2 포드(Pod): 컨테이너를 다루는 기본 단위  
### 6.2.1 포드 사용하기  
쿠버네티스에서 컨테이너 애플리케이션의 기본 단위를 포드라고 부르며, 포드는 1개 이상의 컨테이너로 구성된 컨테이너의 집합이다.  
1개의 포드에는 여러 개의 컨테이너가 존재할 수도 있다. 

Nginx 컨테이너로 구성된 포드를 직접 생성하기 위한 nginx-pod.yaml  
```  
apiVersion: v1  
kind: Pod  
metadata:  
  name: my-nginx-pod  
spec:  
  containers:  
  - name: my-nginx-container  
    image: nginx:latest  
    ports:  
    - containerPort: 80  
      protocol: TCP  
```

1. apiVersion: YAML 파일에서 정의한 오브젝트의 API 버전  
2. kind: 이 리소스의 종류를 나타내며, 생성하려고 하는 것을 입력한다. 
(kind 항목에서 사용할 수 있는 리소스 오브젝트 종류는  ``kubectl api-resources`` 명령어 결과, KIND 항목에서 확인할 수 있다.)  

3. metadata: 라벨, 주석, 이름 등과 같은 리소스의 부가 정보들을 입력한다. 예시에서는 name 항목에서 포드의 고유한 이름을 my-nginx-pod로 설정.  
4. spec: 리소스를 생성하기 위한 자세한 정보를 입력한다.  
예시에서는 포드에서 실행될 컨테이너 정보를 정의하는 containers 항목을 작성한 뒤, 하위 항목인 image에서 사용할 도커 이미지를 지정.  
(name 항목에서는 컨테이너의 이름, ports 항목에서는 Nginx 컨테이너가 사용할 포트인 80)  

작성한 YAML 파일은 ``kubectl apply -f <xx.yaml>`` 명령어로 쿠버네티스에 생성할 수 있다.  
``kubectl get <오브젝트이름>``을 사용하면 특정 오브젝트 목록을 확인할 수 있다.  
(``kubectl get pods`` 명령어는 현재 쿠버네티스에 존재하는 포드의 목록을 출력한다.)  

쿠버네티스 외부 또는 내부에서 접근하려면 서비스라고 하는 쿠버네티스 오브젝트를 따로 생성해야 한다.  
서비스 오브젝트 없이 IP만으로 Nginx 포드에 접근하려면,  
클러스터의 노드 중 하나에 접속한 뒤 Nginx 포드의 IP로 HTTP 요청을 전송하여, 정상적으로 실행 중인지 확인할 수 있다.  

kubectl exec 명령으로 포드의 컨테이너에 명령어를 전달할 수 있다.  
``kubtcl exec -it my-nginx-pod bash``  
-> my-nginx-pod에서 배치 셸을 실행하되, -it 옵션으로 셸을 유지함.  

kubectl logs 명령어로 포드의 로그를 확인할 수있다.  
``kubectl logs my-nginx-pod``  
-> Nginx 서버에 접근했던 기록 확인.  

쿠버네티스의 오브젝트는 kubectl delete -f 명령어로 쉽게 삭제할 수 있다.  
``kubectl delete -f nginx-pod.yaml``  
-> nginx-pod.yaml에 정의된 Nginx 포드를 삭제.  


### 6.2.2 포드 vs 도커 컨테이너  
포드는 컨테이너 IP 주소를 가지고 있어 쿠버네티스 클러스터 내부에서 접근할 수 있고,  
``kubectl exec`` 명령어로 포드 컨테이너 내부로 들어갈 수도 있으며,  
``kubectl logs`` 명령어로 포드의 로그를 확인할 수도 있다.  
이 기능들만 놓고 본다면 docker run으로 생성한 단일 nginx 컨테이너와 크게 다르지 않아 보인다.  

쿠버네티스가 포드를 사용하는 이유는 컨테이너 런타임의 인터페이스 제공 등 여러 가지가 있지만,  
여러 리눅스 네임스페이스를 공유하는 여러 컨테이너들을 추상화된 집합으로 사용하기 위해서다.  

Nginx 포드에 새로운 우분투 컨테이너를 추가한다면, 
우분투 컨테이너가 Nginx 서버를 실행하고 있지 않은데도, 우분투 컨테이너의 로컬호스트에서 Nginx 서버로 접근이 가능하다.  
이는 포드 내의 컨테이너들이 네트워크 네임스페이스 등과 같은 리눅스 네임스페이스를 공유해 사용하기 때문이다.  
(컨테이너 네트워크 타입은 네트워크 네임스페이스를 컨테이너 간에 공유해 사용할 수 있도록 설정하기 때문에 여러 개의 컨테이너가 동일한 네트워크 환경을 가지게 된다.)  

1개의 포드에 포함된 컨테이너들은 여러 개의 리눅스 네임스페이스를 공유한다.  
즉, 포드 내부의 컨테이너들은 네트워크와 같은 리눅스 네임스페이스를 공유한다.  


### 6.2.3 완전한 애플리케이션으로서의 포드  
실제 쿠버네티스 환경에서는 1개의 컨테이너로 구성된 포드를 사용하는 경우가 많다.  
하나의 포드는 하나의 완전한 애플리케이션이기 때문에, 하나의 포드에 2개의 컨테이너가 정의되는 것은 바람직하지 않다.  

그러나 Nginx 컨테이너가 실행되기 위해 부가적인 기능을 필요로 한다면,  
주 컨테이너를 Nginx로 하되, 기능 확장을 위한 추가 컨테이너를 함께 포드에 포함시킬 수 있다.  
이렇게 포드에 정의된 부가적인 컨테이너를 사이드카(sidecar) 컨테이너라고 부르며, 사이드카 컨테이너는 포드 내의 다른 컨테이너와 네트워크 환경 등을 공유하게 된다.  
(포드에 포함된 컨테이너들은 모두 같은 워커 노드에서 함께 실행된다.)  
이러한 구조 및 원리에 따라 포드에 정의된 여러 개의 컨테이너는 하나의 완전한 애플리케이션으로서 동작하게 된다.  


### 6.3 레플리카셋(Replica Set): 일정 개수의 포드를 유지하는 컨트롤러  
### 6.3.1 레플리카셋을 사용하는 이유  
``kubectl delete`` 명령어로 포드를 삭제하면, 그 포드의 컨테이너 또한 삭제된 뒤 쿠버네티스에서 영원히 사라지게 된다.  
실제로 외부 사용자의 요청을 처리해야 하는 마이크로서비스 구조의 포드라면, 여러 개의 동일한 컨테이너를 생성한 뒤 외부 요청이 각 컨테이너에 적절히 분배될 수 있어야 한다.  

기본 단위가 포드이기 때문에, 동일한 여러 개의 포드를 생성해 외부 요청을 각 포드에 분배하는 방식을 사용해야 한다.  
동일한 여러 개의 포드를 생성하는 가장 간단한 방법은 다른 이름을 가지는 여러 개의 포드를 직접 만드는 방식이다.  
이 방식은 적절하지 않은데, 일일이 정의하는 것은 비효율적이며, 포드에 접근하지 못할 때 직접 포드를 삭제하고 다시 생성하지 않는 한 해당 포드는 다시 복구되지 않는다.  

이러한 포드만 YAML 파일에 정의해 사용하는 방식은 여러 가지 한계점이 있으므로, 레플리카셋이라는 쿠버네티스 오브젝트를 함께 사용하는 것이 일반적이다.  
레플리카셋은 정해진 수의 동일한 포드가 항상 실행되도록 관리하며,  
노드 장애 등의 이유로 포드를 사용할 수 없다면 다른 노드에서 포드를 다시 생성한다.  
따라서 동일한 Nginx 포드를 안정적으로 여러 개 실행할 수도 있고, 워크 노드에 장애가 생기더라도 정해진 개수의 포드를 유지할 수 있다.  


### 6.3.2 레플리카셋 사용하기  
Nginx 포드를 생성하는 레플리카셋을 만들기 위한 replicaset-nginx.yaml 파일  

``` 
apiVersion: apps/v1  
kind: ReplicaSet  
metadata:  
  name: replicaset-nginx  
spec:  
  replicas: 3  
  selector:  
    matchLabels:  
      app: my-nginx-pods-label  
  template:  
    metadata:  
      name: my-nginx-pod  
      labels:  
        app: my-nginx-pods-label  
    spec:  
      containers:  
      - name: nginx  
        image: nginx:latest  
        ports:  
        - containerPort: 80  
```

1. spec.replicas: 동일한 포드를 몇 개 유지시킬 것인지 설정  
예시에서는 레플리카셋은 3개의 포드를 새롭게 생성한다.  

2. spec.template: 포드를 생성할 때 사용할 템플릿을 정의. 어떠한 포드를 어떻게 생성할 것인지를 명시하며, 보통 포드 스펙, 또는 포드 템플릿이라고 한다.  

레플리카셋을 삭제하려면 ``kubectl delete -f`` 명령어를 사용하거나, 
``kubectl delete rs`` 명령어를 사용하면 된다.  
(replicasets 대신 rs를 사용할 수 있다.)  
레플리카셋에 의해 생성된 포드 또한 함께 삭제된다.  


### 6.3.3 레플리카셋의 동작 원리  
실제로는 레플리카셋은 포드와 연결돼 있지 않고, 느슨한 연결을 유지하고 있으며, 이러한 느슨한 연결은 포드와 레플리카셋의 정의 중 라벨 셀렉터를 이용해 이뤄진다.  

라벨은 쿠버네티스 리소스의 부가적인 정보를 표현할 수 있을 뿐만 아니라, 서로 다른 오브젝트가 서로를 찾아야 할 때 사용되기도 한다.  
예를들어 레플리카셋은 spec.selector.matchLabel에 정의된 라벨을 통해 생성해야 하는 포드를 찾는다.  
즉, app: my-nginx-pods-label 라벨을 가지는 포드의 개수가 replica 항목에 정의된 숫자인 3개와 일치하지 않으면, 포드를 정의하는 포드 템플릿 항목의 내용으로 포드를 생성한다.  
예를들어, app: my-nginx-pods-label 라벨을 가지는 포드를 미리 생성해 놓은 다음 레플리카셋을 생성한다면, 이미 존재하는 포드를 제외하고 생성된다.  

수동으로 생성한 포드를 삭제할 경우, 라벨셀렉터와 일치하는 포드가 줄어들었기 때문에, 새로운 포드를 생성한다.  

레플리카셋이 생성해 놓은 포드의 라벨을 삭제할 경우에도 새로운 포드를 생성한다.  
라벨을 삭제한 포드는 레플리카셋의 selector.matchLabel 항목의 값과 더 이상 일치하지 않으므로 레플리카셋에 의해 관리되지 않으며,  
직접 수동으로 생성한 포드와 동일한 상태가 된다.  
따라서 레플리카셋을 삭제해도 이 포드는 삭제되지 않으며, 직접 수동으로 삭제해야 한다.  

레플리카셋의 목적은 포드를 생성하는 것이 아닌 일정 개수의 포드를 유지하는 것이다.  


### 6.3.4 레플리케이션 컨트롤러 vs 레플리카셋  
이전 버전의 쿠버네티스에서는 레플리카셋이 아닌 레플리케이션 컨트롤러(Replication Controller)라는 오브젝트를 통해 포드의 개수를 유지했다.  
쿠버네티스의 버전이 올라감에 따라 레플리케이션 컨트롤러는 더 이상 사용되지 않으며(deprecated), 그 대신 레플리카셋이 사용되고 있다.  
차이 중 하나는 표현식(matchExpression) 기반의 라벨 셀렉터를 사용할 수 있다.  
```  
selector:  
    matchExpresssions:  
        - key: app  
          value:  
              - my-nginx-pods-label  
              - your-nginx-pods-label  
          operator: In  
```
 
위 예시는 키가 app인 라벨을 가지고 있는 포드들 중에서, values 항목에 정의된 값들이 존재하는 포드들을 대상으로 하겠다는 의미이다.  
(각 라벨을 가지는 포드 또한 레플리카셋의 관리하에 놓이게 된다.)  
operator에는 In 외에도 NotIn, Exists 등을 사용할 수 있다. 


## 08 인그레스(Ingress)  
실제 운영 환경에서 쿠버네티스를 사용하려면 많은 기능이 필요하다. 예를 들어 네트워크 7계층에서 가상 호스트를 이용해 서비스 요청을 처리하거나, 애플리케이션의 영속적인 데이터를 보관하기 위한 외부 볼륨이 필요할 수도 있다. 또한 여러 명의 개발자 또는 애플리케이션이 함께 사용하는 쿠버네티스 클러스터에서는 보안을 위해 반드시 권한을 관리해야 하며, 특정 포드가 컴퓨팅 자원을 독차지하는 것을 막기 위해 메모리, CPU 사용량의 제한을 위한 체계적인 시스템도 필요하다.  

인그레스는 일반적으로 외부에서 내부로 향하는 것을 지칭한다. 예를들어 인그레스 트래픽은 외부에서 서버로 유입되는 트래픽을 의미하며, 인그레스 네트워크는 인그레스 트래픽을 처리하기 위한 네트워크를 의미한다. 인그레스는 외부요청을 어떻게 처리할 것인지 네트워크 7계층 레벨에서 정의하는 쿠버네티스 오브젝트이다.  

인그레스 오브젝트가 담당할 수 있는 기본적인 기능  
1. 외부 요청의 라우팅: /apple, /apple/red 등과 같이 특정 경로로 들어온 요청을 어떠한 서비스로 전달할지 정의하는 라우팅 규칙을 설정할 수 있다.  
2. 가상 호스트 기반의 요청 처리: 같은 IP에 대해 다른 도메인 이름으로 요청이 도착했을 때, 어떻게 처리할 것인지 정의할 수 있다.  
3. SSL/TLS 보안 연결 처리: 여러 개의 서비스로 요청을 라우팅할 때, 보안 연결을 위한 인증서를 쉽게 적용할 수 있다.  


### 8.1 인그레스를 사용하는 이유  
애플리케이션이 3개의 디플로이먼트로 생성돼 있을 때, 각 디플로이먼트를 외부에 노출해야 한다면 NodePort 또는 LoadBalancer 타입의 서비스 3개를 생성하는 방법이 있다. 각 디플로이 먼트에 대응하는 서비스를 하나씩 연결해주는 방식인데, 서비스마다 세부적인 설정을 할 때 추가적인 복잡성이 발생한다. SSL/TLS 보안 연결, 접근 도메인 및 클라이언트 상태에 기반한 라우팅 등을 구현하려면 각 서비스와 디플로이먼트에 대해 일일이 설정을 해야 하기 때문이다. 인그레스 오브젝트를 사용하면 URL 엔드포인트를 단 하나만 생성함으로써 이러한 번거로움을 쉽게 해결할 수 있다. 라우팅 정의나 보안 연결 등과 같은 세부 설정은 서비스와 디폴로이먼트가 아닌 인그레스에 의해 수행된다. 즉, 외부 요청에 대한 처리 규칙을 쿠버네티스 자체의 기능으로 편리하게 관리할 수 있다.  


### 8.2 인그레스의 구조  
``kubectl get ingress`` 명령어로 인그레스의 목록을 확인할 수 있다.  
``-> (처음사용시)No resources found in default namespace.``  

YAML 파일로 인그레스를 정의해서 생성  
```
apiVersion: networking.k8s.io/v1beta1  
kind: Ingress  
metadata:  
  name: ingress-example  
  annotations:  
    nginx.ingress.kubernetes.io/rewrite-target: /  
    kubernetes.io/ingress.class: "nginx"  
spec:  
  rules:  
  - host: alicek106.example.com                  # [1]  
    http:  
      paths:  
      - path: /echo-hostname                     # [2]  
        backend:  
          serviceName: hostname-service          # [3]  
          servicePort: 80  
```
(apiVersion은 쿠버네티스 오브젝트나 API의 종류 및 성숙도를 나타내는 일종의 카테고리이다.)  
1. host: 해당 도메인 이름으로 접근하는 요청에 대해서 처리 규칙을 적용한다.  
위 예시에서는 alicek106.example.com이라는 도메인으로 접근하는 요청만 처리하지만, 여러 개의 host를 정의해 사용할 수도 있다.  
2. path: 해당 경로에 들어온 요청을 어느 서비스로 전달할 것인지 정의한다.  
위 예시에서는 /echo-hostname이라는 경로의 요청을 backend에 정의된 서비스로 전달한다.  
여러 개의 path를 정의해 경로를 처리할 수도 있다.  
3. serviceName, servicePort: path로 들어온 요청이 전달될 서비스와 포트이다.  
위 예시에서는 /echo-hostname이라는 경로로 들어온 요청을 hostname-service 서비스의 80 포트로 전달한다.  

ingress-example.yaml 파일로 인그레스를 생성한 다음, 인그레스의 목록을 확인  
``kubectl apply -f ingress-example.yaml``  

이슈  
https://kubernetes.io/docs/reference/using-api/deprecation-guide/
...
Ingress  
The extensions/v1beta1 and networking.k8s.io/v1beta1 API versions of Ingress is no longer served as of v1.22.  
...

``kubectl apply -f ingress-example2.yaml``  
```
apiVersion: networking.k8s.io/v1  
kind: Ingress  
metadata:  
  name: ingress-example2  
  annotations:  
    nginx.ingress.kubernetes.io/rewrite-target: /  
    kubernetes.io/ingress.class: "nginx"  
spec:  
  rules:  
  - host: alicek106.example.com                 # [1]  
    http:  
      paths:  
      - path: /echo-hostname                    # [2]  
        pathType: Prefix  
        backend:  
          service:  
            name: hostname-service              # [3]  
            port:  
              number: 80  
```

```
C:\kubernetes\chapter8>kubectl apply -f ingress-example2.yaml  
ingress.networking.k8s.io/ingress-example2 created  

C:\kubernetes\chapter8>  
C:\kubernetes\chapter8>  
C:\kubernetes\chapter8>kubectl get ingress  
NAME               CLASS    HOSTS                   ADDRESS   PORTS   AGE  
ingress-example2   <none>   alicek106.example.com             80      10s  
```
인그레스를 생성했지만, 아무것도 일어나지 않는다. 인그레스는 단지 요청을 처리하는 규칙을 정의하는 선언적인 오브젝트일 뿐, 외부 요청을 받아들일 수 있는 실제 서버가 아니기 때문이다.  
인그레스는 인그레스 컨트롤러라고 하는 특수한 서버에 적용해야만 그 규칙을 사용할 수 있다. 즉, 실제로 외부 요청을 받아들이는 것은 인그레스 컨트롤러 서버이며, 이 서버가 인그레스 규칙을 로드해 사용한다.  

인그레스 컨트롤러 서버는 여러 종류가 있으며, 대표적으로는 쿠버네티스 커뮤니티에서 활발히 사용되고 있는 Nginx 웹 서버 인그레스 컨트롤러가 있다. 쿠버네티스에서 공식적으로 개발되고 있기 때문에 설치를 위한 YAML 파일을 공식 깃허브 저장소에서 직접 내려받을 수 있다.  

Nginx 인그레스 컨트롤러와 관련된 모든 리소스를 한번에 설치할 수 있는 명령어  
```
kubectl apply -f \  
https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/aws/deploy.yaml  
```  

ingress-nginx 네임스페이스의 디플로이먼트와 포드를 확인하여 Nginx 웹 서버 생성돼어 있음을 확인  
``kubectl get pods,deployment -n ingress-nginx``  

외부에서 Nginx 인그레스 컨트롤러에 접근하기 위한 서비스 생성 확인  
``kubectl get svc -n ingress-nginx``  

인그레스의 종착점이 될 테스트용 디플로이먼트와 서비스를 생성해 최종적으로 인그레스의 동작 여부 확인  
``kubectl apply -f hostname-deployment.yaml``  
``kubectl apply -f hostname-service.yaml``  
``kubectl get pods,services``  
```
C:\kubernetes\chapter8>kubectl get pods,services  
NAME                                       READY   STATUS             RESTARTS   AGE  
pod/hostname-deployment-78db489b5c-fzbj9   1/1     Running            0          31m  
pod/hostname-deployment-78db489b5c-gkxr7   1/1     Running            0          32m  
pod/hostname-deployment-78db489b5c-jpr7l   1/1     Running            0          31m  

NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE  
service/hostname-service         ClusterIP   10.109.75.105   <none>        80/TCP     32m  
```
인그레스 컨트롤러에 의해 요청이 최종적으로 도착할 디플로이먼트의 서비스는 어떤 타입이든지 상관은 없다.  
다만 굳이 외부에 서비스를 노출할 필요가 없다면, ClusterIP 타입을 사용하는 것이 좋다.  
위 예시에서는 hostname-service라는 이름의 서비스는 ClusterIP타입으로 생성했다.  

Nginx 인그레스 컨트롤러의 /echo-hostname으로 요청을 전송  
- NodePort 타입으로 서비스를 생성했을 경우  
```
kubectl describe pods hostname-deployment-78db489b5c-fzbj9  
curl --resolve alicek106.example.com:31000:172.17.0.4 alicek106.example.com:31000/echo-hostname  
```


인그레스를 사용하는 방법  
1. 공식 깃허브에서 제공되는 YAML 파일로 Nginx 인그레스 컨트롤러를 생성한다.  
2. Nginx 인그레스 컨트롤러를 외부로 노출하기 위한 서비스를 생성한다.  
3. 요청 처리 규칙을 정의하는 인그레스 오브젝트를 생성한다.  
- 인그레스를 생성하면 인그레스 컨트롤러는 자동으로 인그레스를 로드해 Nginx 웹 서버에 적용한다. 이를 위해 Nginx 인그레스 컨트롤러는 항상 인그레스 리소스의 상태를 지켜보고 있으며, 기본적으로 모든 네임스페이스의 인그레스 리소스를 읽어와 규칙을 적용한다.  
4. Nginx 인그레스 컨트롤러로 들어온 요청은 인그레스 규칙에 따라 적절한 서비스로 전달된다.  
- 테스트용 인그레스에서는 /echo-hostname이라는 경로로 들어온 요청을 hostname-service라는 서비스의 80 포트로 전달했다. 요청이 실제로 hostname-service라는 서비스로 전달되는 것은 아니며, 서비스에 의해 생성된 엔드포인트로 요청을 직접 전달한다.  
``kubectl get endpoints``


### 8.3 인그레스의 세부 기능: annotation을 이용한 설정  
인그레스는 YAML 파일의 주석 항목을 정의함으로써 다양한 옵션을 사용할 수 있다.  
```
apiVersion: networking.k8s.io/v1beta1  
kind: Ingress  
metadata:  
  name: ingress-example  
  annotations:  
    nginx.ingress.kubernetes.io/rewrite-target: /  # [2] hostname-service의 /로 전달된다.  
    kubernetes.io/ingress.class: "nginx"  
spec:  
  rules:  
  - host: alicek106.example.com  
    http:  
      paths:  
      - path: /echo-hostname                     # [1] /echo-hostname 경로로 들어온 요청을  
        backend:  
          serviceName: hostname-service  
          servicePort: 80  
```
kubernetes.io/ingress.class는 해당 인그레스 규칙을 어떤 인그레스 컨트롤러에 적용할 것인지를 의미한다.  
nginx.ingress.kubernetes.io/rewrite-target는 인그레서에 정의된 경로로 들어오는 요청을 rewrite-target에 설정된 경로로 전달한다. 예를 들어, Nginx 인그레스 컨트롤러로 /echo-hostname으로 접근하면 hostname-service에는 / 경로로 전달된다.  
단, rewrite-target은 /echo-hostname이라는 경로로 시작하는 모든 요청을 hostname-service의 / 로 전달한다.  
예를 들어 /echo-hostname/alice/bob이라는 경로로 요청을 보내도 똑같이  /로 전달된다.  


### 8.4 Nginx 인그레스 컨트롤러에 SSL/TLS 보안 연결 적용  
인그레스의 장점 중 하나는 쿠버네티스의 뒤쪽에 있는 디플로이먼트와 서비스가 아닌, 앞쪽에 있는 인그레스 컨트롤러에서 편리하게 SSL/TLS 보안 연결을 설정할 수 있다는 것이다.  
즉, 인그레스 컨트롤러 지점에서 인증서를 적용해 두면, 요청이 전달되는 애플리케이션에 대해 모두 인증서 처리를 할 수 있다.  
따라서 인그레스 컨트롤러가 보안 연결을 수립하기 위한 일졸의 관문(Gateway) 역할을 한다고 볼 수 있다.  

Nginx 인그레스 컨트롤러 또한 인증서를 통한 보안 연결 기능을 제공하기 때문에 어렵지 않게 보안 연결을 설정할 수 있다.  
보안 연결에 사용할 인증서와 비밀키 생성  
```
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \  
-keyout tls.key -out tls.crt -subj "/CN=alicek106.example.com/0=alicek106  
```
/CN에는 Nginx 인그레서 컨트롤러에 접근하기 위한 Public DNS 이름을 입력해야 한다.  
위 예시는 Nginx 인그레스 컨트롤러와 연결된 서비스에  alicek106.example.com이라는 도메인으로 접근한다고 가정한 것이며,  
AWS에서 생성되어 동적인 도메인 이름을 할당받은 클래식 로드 밸런서라면 /CN=*.qp-northeast-2.elb.amazonaws.com처럼 사용하는 것도 가능하다.  

위 명령어로 tls.key라는 비밀키와 tls.crt라는 인증서가 생성된다. 이 파일들을 통해 tls 타입의 시크릿을 생성한다.  
``kubectl create secret tls tls-secret --key tls.key --cert tls.crt``  
...

인그레스 설정에 TLS 옵션을 추가해 적용한다.  
```
apiVersion: networking.k8s.io/v1beta1  
kind: Ingress  
metadata:  
  name: ingress-example  
  annotations:  
    nginx.ingress.kubernetes.io/rewrite-target: /  
    kubernetes.io/ingress.class: "nginx"  
spec:  
  tls:  
  - hosts:  
    - alicek106.example.com            # 여러분의 도메인 이름을 입력해야 합니다.  
    secretName: tls-secret  
  rules:  
  - host: alicek106.example.com          # 여러분의 도메인 이름을 입력해야 합니다.  
    http:  
      paths:  
      - path: /echo-hostname  
        backend:  
          serviceName: hostname-service  
          servicePort: 80  
```
spec.tls.hosts 항목에서는 보안 연결을 적용할 도메인 이름을, spec.tls.secretName은 앞서 생성했던 tls 타입의 시크릿 이름을 입력한다.  
이는 alicek106.example.com이라는 도메인 이름으로 접근하는 요청에 대해 tls-secret 시크릿의 인증서로 보안 연결을 수립하겠다는 뜻이다.  

인그레스를 생성한 뒤 Nginx 인그레스 컨트롤러로 요청을 보내고 데이터를 반환하는 것을 확인한다.  
``kubectl apply -f ingress-tls.yaml``  
``curl https://alicek106.example.com/echo-hostname -k``  
(curl -k 옵션은 신뢰할 수 없는 인증서로 보안 연결을 하기 위함)  


### 8.5 여러 개의 인그레스 컨트롤러 사용하기  
하나의 쿠버네티스 클러스터에서 반드시 하나의 인그레스 컨트롤러를 사용해야 하는 것은 아니다. Nginx 인그레스 컨트롤러는 기본적으로 nginx라는 이름의 클래스를 가지고 있으며, 이 설정을 변경함으로써 여러 개의 Nginx 인그레스 컨트롤러를 사용할 수 있고, 인그레스 규칙을 선택적으로 적용할 수도 있다.  

Nginx 인그레스 컨트롤러를 생성할 때 --ingress-class라는 옵션 및 alicek106-nginx라는 값을 설정하면, 이전에 생성했던 인그레스 규칙은 더 이상 Nginx 인그레스 컨트롤러에 적용되지 않는다.  
따라서 kubernetes.io/ingress.class 주석을 alicek106-nginx로 수정해줘야만 Nginx 인그레스 컨트롤러가 해당 인그레스의 규칙을 정상적으로 로드해 적용한다.  
```
apiVersion: networking.k8s.io/v1beta1  
kind: Ingress  
metadata:  
  name: ingress-example  
  annotations:  
    nginx.ingress.kubernetes.io/rewrite-target: /  
    kubernetes.io/ingress.class: "alicek106-nginx"  
```



## 10 보안을 위한 인증과 인가: ServiceAccount RBAC  
쿠버네티스는 보안 측면에서도 다양한 기능을 제공하고 있는데, 그중에서 가장 자주 사용되는 것은 RBAC(Role Based Access Control)를 기반으로 하는 서비스 어카운트라는 기능이다.  
서비스 어카운트는 사용자 또는 애플리케이션 하나에 해당하며, RBAC라는 기능을 통해 특정 명령을 실행할 수 있는 권한을 서비스 어카운트에 부여한다.  
권한을 부여받은 서비스 어카운트는 해당 권한에 해당하는 기능만 사용할 수 있게 된다.  
(간단히 생각해서 리눅스에서 root 유저와 일반 유저를 나누는 기능을 쿠버네티스에서도 유사하게 사용할 수 있다.)  

kubectl 명령어를 사용해왔던 권한은 사실 최상위에 해당하는, 마치 리눅스의  root 사용자와 같은 권한을 가지고 있다.  
따라서 사용자에게 필요한 권한만을 최소한으로 부여함으로써 실행할 수 있는 기능을 제한하는 것이 바람직하다.  


### 10.1 쿠버네티스의 권한 인증 과정  
쿠버네티스 컴포넌트들은 kube-system 네임스페이스에서 실행되고 있으므로 직접 목록을 확인할 수 있다.  
``kubectl get pods -n kube-system``  

kubectl 명령어를 사용해 쿠버네티스의 기능을 실행하면 쿠버네티스 내부에서 일어나는 복잡한 절자  
1. kubectl 명령어는 쿠버네티스 API 서버의 HTTP 핸들러에 요청을 전송한다.  
2. API 서버는 해당 클라이언트가 쿠버네티스의 사용자가 맞는지(Authentication: 인증), 해당 기능을 실행할 권한이 있는지(Authorization: 인가) 확인한다.  
인증과 인가에는 서비스 어카운트 외에도 서드파티 인증(Open ID Connect: OAuth), 인증서 등과 같이 다양한 방법이 사용될 수 있다.  
3. 그 뒤에는 어드미션 컨트롤러라는 별도의 단계를 거친 뒤 비로소 요청받은 기능을 수행한다.  

설치 도구를 이용해 쿠버네티스를 설치할 때 설치 도구가 자동으로 kubectl이 관리자 권한을 갖도록 설정해 둔다.  
(설정은 ``~/.kube/config``라는 파일에서 확인할 수 있고, kubectl을 사용할 때는 기본적으로  ``~/.kube/config``라는 파일에 저장된 설정을 읽어 들여 쿠버네티스 클러스터를 제어한다.)  
users 항목에는 인증을 위한 정보를, clusters 항목에는 클러스터에 접근하기 위한 정보를 저장한다.  

기본적으로 설정된 ``~/.kube/config`` 파일에서는 인증서 키 쌍을 사용해 API 서버에 인증하지만, 이 인증 방법은 비교적 절차가 복잡하고 관리하기가 어렵기 때문에 자주 사용하는 방법은 아니다.  
쿠버네티스에서는 인증을 위해 인증서 키 쌍뿐만 아니라 여러 가지 방법을 사용할 수 있으며, 그중 하나가 서비스 어카운트이다.  


### 10.2 서비스 어카운트와 롤(Role), 클러스터 롤(ClusterRole)  
서비스 어카운트는 체계적으로 권한을 관리하기 위한 쿠버네티스 오브젝트이다.  
네임스페이스에 속하는 오브젝트로, serviceaccount 또는 sa라는 이름으로 사용할 수 있다.  
``kubectl get serviceaccount``  
``kubectl get sa #sa라는 이름으로도 사용 가능``  

각 네임스페이스에는 기본적으로 default라는 이름의 서비스 어카운트가 존재한다.  
kubectl create나 delete를 통해 간단하게 서비스 어카운트를 생성하거나 삭제할 수 있다.  
``kubectl create sa alicek106``  

생성한 alicek106이라는 이름의 서비스 어카운트를 이용해 kubectl 명령어를 사용하는 예  
(--as 옵션을 사용하면 임시로 특정 서비스 어카운트를 사용할 수 있다.)  
``kubectl get services --as system:serviceaccount:default:alicek106``  
(--as 옵션에 사용된 system:serviceaccount는 인증을 위해 서비스 어카운트를 사용한다는 것을 나타내며, default:alicek106은 default 네임스페이스의 alicek106 서비스 어카운트를 의미한다.)  
```
Error from server (Forbidden): services is forbidden: User "system:serviceaccount:default:alicek106" cannot list resource "services" in API group "" in the namespace "default"
```

방금 생성한 alicek106 서비스 어카운트로 서비스의 목록을 조회하면 API서버로부터 에러가 반환된다. 이 서비스 어카운트는 default 네임스페이스에서 서비스 목록을 조회할 수 있는 권한이 아직 부여되지 않았기 때문이다.  


쿠버네티스에 권한을 부여하는 방법에는 크게 두 가지가 있는데, 롤과 클러스터 롤을 이용해 권한을 설정하는 것이다.  
이 두가지는 부여할 권한이 무엇인지를 나타내는 쿠버네티스 오브젝트이다.    
1. 롤(Role)  
롤은 네임스페이스 속하는 오브젝트이므로, 디플로이먼트나 서비스처럼 네임스페이스에 속하는 오브젝트들에 대한 권한을 정의할 때 쓰인다.  
(예를들어 '디플로이먼트를 생성할 수 있다'라는 것도 하나의 롤이 될 수 있고,  
'서비스 목록을 조회한다'라는 것도 롤이 될 수 있다.)  

현재 네임스페이이스의 롤 목록 출력  
``kubectl get role``  
-> No resources found in default namespace  

2. 클러스터 롤(Cluter Role)   
클러스터 롤은 네임스페이스에 속하지 않고(전역적이다), 클러스터 단위의 권한을 정의할 때 사용한다.  
예를 들어 '퍼시스턴트 볼륨의 목록을 조회할 수 있다'라는 권한은 클러스트 롤로 정의할 수 있다.  
클러스터 전반에 걸친 기능을 사용하기 위해서도 클러스터 롤을 정의할 수 있으며,  
여러 네임스페이스에서 반복적으로 사용되는 권한을 클러스터 롤로 만들어 재사용하는 것도 가능하다.  

클러스터 자체에 존재하는 모든 클러스터 롤의 목록을 출력  
``kubectl get clusterrole``  
(쿠버네티스에서 관리자 권한으로 모든 기능을 사용할 수 있는 cluster-admin이라는 클러스터 롤을 확인할 수 있다.)  

서비스의 목록을 읽을 수 있는 롤을 정의하는 YAML 파일  
``service-reader-role.yaml``  
```
apiVersion: rbac.authorization.k8s.io/v1  
kind: Role  
metadata:  
  namespace: default                 # 롤이 생성될 네임스페이스  
  name: service-reader               # 롤의 이름  
rules:  
- apiGroups: [""]                    # 1. 대상이 될 오브젝트의 API 그룹  
  resources: ["services"]            # 2. 대상이 될 오브젝트의 이름  
  verbs: ["get", "list"]             # 3. 어떠한 동작을 허용할 것인지 명시  
```
apiGroups: 어떠한 API 그룹에 속하는 오브젝트에 대해 권한을 지정할지 설정  
""는 포드, 서비스 등이 포함된 코어 API 그룹을 의미.  
``kubectl api-resources`` 명령어를 사용하면 특정 쿠버네티스 오브젝트가 어떤 API 그룹에 속하는지 확인할 수 있다.  
```
pods                              po           v1                                     true         Pod  
services                          svc          v1                                     true         Service  
deployments                       deploy       apps/v1                                true         Deployment  
```
포드나 서비스 등은 코어 API 그룹에 속하기 때문에 표시되지 않지만, 디플로이먼트나 레플리카셋 등은 apps라는 이름의 API 그룹에 속한다.  

resources: 어떠한 쿠버네티스 오브젝트에 대해 권한을 정의할 것인지 입력.  

verbs: 이 롤을 부여받은 대상이 resources에 지정된 오브젝트들에 대해 어떤 동작을 수행할 수 있는지 정의  
(위 예시에서는 get과 list 동작을 명시했으므로 ``kubectl get services`` 명령어로 개별 서비스의 정보를 가져오거나 모든 서비스 목록을 확인할 수 있도록 권한이 부여된다.)  


즉, 코어 API 그룹에 속하는 서비스 리소스에 대해 get과 list를 실행할 수 있다.  
``kubectl apply -f service-reader-role.yaml``  
-> role.rbac.authorization.k8s.io/service-reader created  
``kubectl get roles``  


이 롤을 특정 대상에게 부여하려면 롤바인딩이라는 오브젝트를 통해 특정 대상과 롤을 연결해야 한다.  
``rolebinding-service-reader.yaml``
```
apiVersion: rbac.authorization.k8s.io/v1  
kind: RoleBinding  
metadata:  
  name: service-reader-rolebinding  
  namespace: default  
subjects:  
- kind: ServiceAccount                 # 권한을 부여할 대상이 ServiceAccount  
  name: alicek106                      # alicke106이라는 이름의 서비스 어카운트에 권한을 부여  
  namespace: default  
roleRef:  
  kind: Role                           # Role에 정의된 권한을 부여  
  name: service-reader                 # service-reader라는 이름의 Role을 대상(subjects)에 연결  
  apiGroup: rbac.authorization.k8s.io  
```
-> alicek106이라는 이름의 서비스 어카운트는 service-reader 롤에 정의된 권한을 사용할 수 있게 된다.  
``kubectl apply -f rolebinding-service-reader.yaml``  
-> rolebinding.rbac.authorization.k8s.io/service-reader-rolebinding created  
``kubectl get services --as system:serviceaccount:default:alicek106``  

하나의 롤은 여러 개의 롤 바인딩에 의해 참조될 수 있고, 하나의 서비스 어카운트는 여러 개의 롤 바인딩에 의해 권한을 부여받을 수도 있다.  
즉, 롤은 권한을 부여하기 위한 일종의 템플릿과 같은 역할을, 롤 바인딩은 롤과 서비스 어카운트를 연결하기 위한 중간 다리 역할을 하는 셈이다.  


노드의 목록을 출력하기 위한 클러스터 롤 생성 예시  
``nodes-reader-clusterrole.yaml``
```
apiVersion: rbac.authorization.k8s.io/v1  
kind: ClusterRole  
metadata:  
  namespace: default  
  name: nodes-reader  
rules:  
- apiGroups: [""]  
  resources: ["nodes"]  
  verbs: ["get", "list"]  
```
``kubectl apply -f nodes-reader-clusterrole.yaml``  
-> clusterrole.rbac.authorization.k8s.io/nodes-reader created  
``kubectl describe clusterrole nodes-reader``  

클러스터 롤 바인딩 생성  
``clusterrolebinding-nodes-reader.yaml``  
```
apiVersion: rbac.authorization.k8s.io/v1  
kind: ClusterRoleBinding  
metadata:  
  name: nodes-reader-clusterrolebinding  
  namespace: default  
subjects:  
- kind: ServiceAccount  
  name: alicek106  
  namespace: default  
roleRef:  
  kind: ClusterRole  
  name: nodes-reader  
  apiGroup: rbac.authorization.k8s.io  
```
``kubectl apply -f clusterrolebinding-nodes-reader.yaml``  
-> clusterrolebinding.rbac.authorization.k8s.io/nodes-reader-clusterrolebinding created  

nodes-reader라는 이름의 클러스터 롤이 서비스 어카운트와 연결됬으므로 노드의 목록 정상 출력 확인    
``kubectl get nodes --as system:serviceaccount:default:alicek106``  



### 10.3 쿠버네티스 API 서버에 접근  
### 10.3.1 서비스 어카운트의 시크릿을 이용해 쿠버네티스 API 서버에 접근  
kubeadm의 경우 쿠버네티스의 마스터 IP와 6443 포트로, GKE나 kops의 경우 443 포트로 접근하면 API 서버에 연결할 수 있다.  
마스터 노드에 SSH로 직접 접속할 수 있다면 SSH로 접속한 뒤 localhost로 요청을 보내도 되지만,  
원격에서 마스터 서버에 접근하고 싶다면 ~/.kube/config 파일에서 server 항목을 찾아 해당 주소로 요청을 보내도 된다.  
단, 쿠버네티스 API 서버는 기본적으로 HTTPS 요청만 처리하도록 설정돼 있으며, 기본적으로 보안 연결을 위해 스스로 사인한(self-signed) 인증서를 사용한다는 점에 유의해야 한다.  

쿠버네티스의 API 서버로 요청이 전송됐지만 401 에러와 함께 API 요청 실패 확인  
(401 메시지는 일반적으로 인증되지 않은 사용자를 의미)  
``curl https://localhost:6443 -k``  
```
{  
  "kind": "Status",  
  "apiVersion": "v1",  
  "metadata": {  
      
  },  
  "status": "Failure",  
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",  
  "reason": "Forbidden",  
  "details": {  
      
  },  
  "code": 403  
}  
```

따라서 API 서버에 접근하려면 별도의 인증 정보를 HTTP 페이로드에 포함시켜 REST API 요청을 전송해야 한다.  
이를 위해 쿠버네티스는 서비스 어카운트를 위한 인증 정보를 시크릿에 저장한다.  
서비스 어카운트를 생성하면 이에 대응하는 시크릿이 자동으로 생성되며, 해당 시크릿은 서비스 어카운트를 증명하기 위한 수단으로 사용된다.  
시크릿의 목록을 출력해 보면 서비스 어카운트의 이름으로 시작하는 시크릿이 존재한다.  
``kubectl get secrets``  
``kubectl describe``  

서비스 어카운트와 연결된 시크릿에는 ca.crt, namespace, token 총 3개의 데이터가 저장돼 있다.  
ca.crt는 쿠버네티스 클러스터의 공개 인증서를, namespace는 해당 서비스 어카운트가 존재하는 네임스페이스를 저장하고 있다.  
``kubectl describe secret alicek106-token-7l2br``  

token 데이터는 쿠버네티스 API 서버와의 JWT(JSONWeb Token) 인증에 사용된다.  
따라서 API 서버의 REST API 엔드포인트로 요청을 보낼 때 token의 데이터를 함께 담아서 보내면 서비스 어카운트라는 것을 인증할 수 있다.  

API 서버로 token 데이터를 함께 전송하기 위해 서비스 어카운트와 연결된 시크릿으로부터 token 데이터 가져오기  
(시크릿의 token 데이터를 base64로 디코딩한 다음, 임시로 셸 변수에 저장해 둔다.)  
``export secret_name=alicek106-token-7l2br``  
``export decoded_token=$(kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d)``  
``echo $decoded_token``  

디코드된 token 데이터를 HTTP 페이로드의 Bearer 헤더에 담아서 다시 API 요청을 보내 정상 응답 반환 확인  
``curl https://localhost:6443/apis --header "Authorization: Bearer $decoded_token" -k``  



API 서버의 몇몇 경로들은 기본적으로 서비스 어카운트가 접근할 수 없도록 제한돼 있다.  
예를 들어 서비스 어카운트의 토큰을 이용해 /logs나 /metrics에 접근하면 권한이 없다는 오류가 반환된다.  
이때 클러스터 롤을 사용하면 이러한 URL에도 접근할 수 있도록 권한을 부여할 수 있다.  
(/metrics와 /logs에 접근할 수 있는 권한을 정의)  
``nonresource-url-clusterrole.yaml``  
```
apiVersion: rbac.authorization.k8s.io/v1beta1  
kind: ClusterRole  
metadata:  
  name: api-url-access  
rules:  
- nonResourceURLs: ["/metrics", "/logs"]  
  verbs: ["get"]  
...  
```


### 10.3.2 클러스터 내부에서 kubernetes 서비스를 통해 API 서버에 접근  
쿠버네티스 클러스터 내부에서 실행 중인 포드는 default 네임스페이스의 kubermetes 서비스를 통해 API 서버에 접근할 수 있다.  
따라서 포드는 kubernetes.default.svc라는 DNS 이름을 통해 쿠버네티스 API를 사용할 수 있다.  
(``kubectl get svc``)  

포드 내부에서 kubernetes라는 이름의 서비스에 접근한다고 해서 특별한 권한이 따로 주어지는 것은 아니고,  
서비스 어카운트에 부여되는 시크릿 토큰을 HTTP 요청에 담아 kubernetes 서비스에 전달해야만 인증과 인가를 진행할 수 있다.  
``kubectl run -i --tty --rm debug --image=alicek106/ubuntu:curl --restart=Never -- bash``  
``curl https://kubernetes -k``  
``https://kubernetes.github.io/ingress-nginx/``  

쿠버네티스는 포드를 생성할 때 자동으로 서비스 어카운트의 시크릿을 포드 내부에 마운트한다.  
디플로이먼트나 포드도 모두 서비스 어카운트의 시크릿을 자동으로 내부에 마운트하고 있다.  
포드를 생성하기 위한 YAML 스펙에 아무런 설정을 하지 않으면 자동으로 default 서비스 어카운트의 시크릿을 포드 내부에 마운트한다.  

시크릿의 데이터는 기본적으로 포드의 /var/run/secrets/kubernetes.io/serviceaccount 경로에 마운트된다.  
해당 경로의 파일을 확인해보면 시크릿의 데이터가 각각 파일로 존재하는데,  
만약 포드 내부에서 API 서버에 접근해야 한다면 token 파일에 저장된 내용을 읽어와 사용하면 된다.  

포드를 생성할 수 있는 권한은 반드시 신뢰할 수있는 사용자에게만 부여하는 것이 좋다.  
포드를 생성할 때 serviceAccountName 항목에 특정 서비스 어카운트의 이름을 명시하면  
포드 내부에서 해당 서비스 어카운트에 연결된 시크릿을 읽을 수 있기 때문이다.  


### 10.3.3 쿠버네티스 SDK를 이용해 포드 내부에서 API 서버에 접근  
API 서버에 접근하기 위해서 HTTP 요청으로 REST API를 사용해도 되지만, 포드 내부에서 실행되는 애플리케이션이라면 특정 언어로 바인딩된 쿠버네티스 SDK를 활용하는 프로그래밍 방식을 더 많이 사용할 것이다.  
특정 서비스 어카운트의 시크릿을 마운트하도록 설정된 포드 내부에서 쿠버네티스 SDK를 사용해 쿠버네티스 API 서버에 접근하는 흐름  
(쿠버네티스 SDK를 포드에서 사용할 때의 인증 흐름)  
1. 서비스 어카운트 생성 및 권한 부여  
롤과 롤 바인딩을 통해 특정 서비스 어카운트에 권한이 부여돼 있어야 한다.  
```
kubectl create sa alicek106  
kubectl apply -f service-reader-role.yaml  
kubectl apply -f rolebinding-service-reader.yaml  
```


2. 포드를 생성하는 YAML에서 serviceAccountName을 명시  
alick106 서비스 어카운트의 시크릿이 마운트된 포드를 생성하는 예  
``kubectl apply -f sa-pod-python-sdk.yaml``  
```
apiVersion: v1  
kind: Pod  
metadata:  
  name: k8s-python-sdk  
spec:  
  serviceAccountName: alicek106  
  containers:  
  - name: k8s-python-sdk  
    image: alicek106/k8s-sdk-python:latest  
```
-> pod/k8s-python-sdk created


3. 시크릿이 포드 내부에 마운트됨  
(포드 내부에 마운트된 alicek106의 시크릿을 확인)  
``kubectl exec -it k8s-python-sdk bash``  
```
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.  
Error from server (BadRequest): pod k8s-python-sdk does not have a host assigned  
```
-> 다른방법 시도  
``docker build . -t alicek106/python-vim-devel:test``  
``ls /var/run/secrets/kubernetes.io/serviceaccount/``  


4. 파이썬 SDK로 쿠버네티스 API 서버에 접근  
```
from kubernetes import client, config  
config.load_incluster_config() # 1  

try:  
  print('Trying to list service..')  
  result = client.CoreV1Api().list_namespaced_service(namespace='default') # 2  
  for item in result.items:  
    print('-> {}'.format(item.metadata.name))  
except client.rest.ApiException as e:  
  print(e)  

print('----')  

try:  
  print('Trying to list pod..')  
  result = client.CoreV1Api().list_namespaced_pod(namespace='default') # 3  
  for item in result.items:  
    print(item.metadata.name)  
except client.rest.ApiException as e:  
  print(e)  
```

- config.load_incluster_config(): 포드 내부에 마운트된 서비스 어카운트의 토큰과 인증서(ca.crt) 파일을 읽어 들여 인증 및 인가 작업을 수행한다.  
- client.CoreV1Api().list_namespaced_service(namespace='default'): CoreV1 그룹의 API를 이용해 특정 네임스페이스의 서비스 목록을 출력한다.  
- client.CoreV1Api().list_namespaced_pod(namespace='default'): CoreV1 그룹의 API를 이용해 특정 네임스페이스의 포드 목록을 출력한다.  

API 그룹은 API 용도와 종류에 따라 분류한 것으로, 이러한 분류는 ``kubectl api-resources`` 명령어로 확인할 수 있다.  
포드나 서비스 등은 코어 API 그룹에 해당하는 client.CoreV1Api()를 통해 함수를 사용할 수 있으나,  
디플로이먼트는 apps라는 이름의 API그룹에 속하기 때문에 다른 함수를 사용해야 한다.  
``client.AppsV1Api.list_namespaced_deployment(namespace='default')``  


파이썬 소스코드를 실행하면 서비스의 목록은 정상적으로 출력되지만, 포드의 목록을 출력하는 함수는 에러를 반환한다.  
포드 내부에 마운트된 시크릿의 서비스 어카운트는 포드의 목록을 출력할 수 있는 권한을 롤이나 클러스터 롤을 통해 부여받지 않았기 때문이다.  
``python3 list-service-and-pod.py``  



### 11.2.4 Taints와 Tolerations 사용하기  
#### Taints와 Tolerrations를 이용한 포드 스케줄링  
쿠버네티스에서는 라벨 외에도 Taints라는 방법을 이용해 포드를 할당할 노드를 선택할 수 있다.  
Taints라는 이름이 의미하는 것처럼 특정 노드에 얼룩(Taint)을 지정함으로써 해당 노드에 포드가 할당되는 것을 막는 기능이다.  
하지만 해당 Taints에 대응하는 용인(Tolerations)을 포드에 설정하면 Taints가 설정된 노드에도 포드를 할당할 수 있다.  
즉, 노드에  얼룩이 졌지만, 이를 용인할 수 있는 포드만 해당 노드에 할당할 수 있다.  

Taints의 종류는 다앙하고, kubectl 명령어로 직접 노드에 설정하거나 해제할 수 있다. Taints는 라벨과 비슷하게 키=값 형태로 사용한다.  
``예시: kubetcl taint nodes nodename key=value:effect``  
``kubectl taint nodes k8s-m alicek106/my-taint=dirty:NoSchedule``  
-> node/k8s-m tainted  

``노드의 Taints를 삭제할 때는 라벨과 마찬가지로 -(대시)를 뒤에 붙이면 된다.``  
``kubectl taint nodes nodename key:effect-``  
``kubectl taint nodes k8s-m alicek106/my-taint:NoSchedule-``  
-> node/k8s-m untainted  

라벨과 한 가지 다른 점은 key=value 뒤에 effect(Taint 효과)를 추가로 명시한다.  
Taint 효과는 Taint가 노드에 설정됐을 때 어떠한 효과를 낼 것인지 결정한다.  
Taint 효과에는 NoSchedule(포드를 스케줄링하지 않음), NoExecute(포드의 실행 자체를 허용하지 않음), PreferNoSchedule(가능하면 스케줄링하지 않음) 총 3가지가 있다.  
(PreferSchedule은 NoSchedule의 소프트 제한 버전이다.)  
(위 예시에서는 Noschedule을 설정했기 때문에 일반적인 포드는 해당 노드에 스케줄링되지 않는다.)  

alicek106/my-taint=dirty:NoSchedule이라는 Taint가 설정된 노드에도 포드를 할당하기 위한 YAML 파일  
``toleration-test.yaml``  
```  
apiVersion: v1  
kind: Pod  
metadata:  
  name: nginx-toleration-test  
spec:  
  tolerations:  
  - key: alicek106/my-taint  
    value: dirty  
    operator: Equal          # alicek106/my-taint 키의 값이 dirty이며 (Equal)  
    effect: NoSchedule       # Taint 효과가 NoSchedule인 경우 해당 Taint를 용인.  
  containers:  
  - name: nginx  
    image: nginx:latest  
```  
(alicek106/my-taint=dirty:NoSchedule이라는 Taint를 용인할 수 있을 뿐, 해당 Taint가 설정된 노드에 반드시 포드를 할당한다는 의미는 아니다.)  

쿠버네티스는 기본적으로 다양한 Taint를 노드에 설정한다.  
대표적인 예로 마스터 노드에 기본적으로 설정된 Taint이다.  
지금까지 포드를 생성하면 기본적으로 마스터 노드가 아닌 워커 노드에 할당됐는데,  
이는 쿠버네티스가 기본적으로 마스터 노드에 Taints를 설정함으로써 포드가 할당되는 것을 방지하기 때문이다.  

마스터 노드의 정보 출력  
``kubectl describe node k8s-m``  
...  
CreationTimestamp:  Sat, 23 Oct 2021 04:59:14 +0000  
Taints:             node-role.kubernetes.io/master:NoSchedule  
Unschedulable:      false  
...  
(Unschedulable: false는 스케줄링의 대상이 되는 노드라는 것을 의미한다. 즉, 쿠버네티스 스케줄러가 스케줄링 시 해당 노드를 포함해 고려한다.)  
따라서 마스터 노드 또한 워커 노드와 마찬가지로 포드가 할당될 수 있는 노드이지만, Taints가 설정돼 있기 때문에 일반적인 포드들은 지금까지 할당되지 않았던 것이다.  

마스터 노드에 설정된 Taint 또한 Toleration을 이용해 용인할 수 있다.  
마스터 노드를 스케줄링 대상으로 간주하려면 포드의 YAML파일에 마스터 노드의 Toleration을 정의하면 된다.  
단, 마스터 노드에는 일반적으로 API 서버와 같은 핵심 컴포넌트만 실행하는 것이 바람직하다.  
``toleration-master.yaml``  
```  
apiVersion: v1  
kind: Pod  
metadata:  
  name: nginx-master-toleration  
spec:  
  tolerations:  						#
  - key: node-role.kubernetes.io/master  
    effect: NoSchedule  
    operator: Equal  
    value: ""  
  nodeSelector:  
    node-role.kubernetes.io/master: ""   # 마스터 노드에서도 포드가 생성되도록 지정합니다.  
  containers:  
  - name: nginx  
    image: nginx:latest  
```  


마스터 노드에서 실행 중인 포드에 설정돼 있는 Toleration 확인  
``kubectl get pods -n kube-system | grep api``  
-> kube-apiserver-k8s-m  	#API 서버 포드의 이름  
``kubectl describe pod kube-apiserver-k8s-m -n kube-system``  
...  
QoS Class:         Burstable  
Node-Selectors:    <none>  
Tolerations:       :NoExecute op=Exists  
Events:            <none>  
...  
(API 서버 포드에는 :NoExecute라는 Toleration이 설정돼 있는데, 이 Toleration은 Taint의 키와 값이 무엇이든지 상관없이 모든 :NoExecute 종류의 Taint를 용인할 수 있음을 의미한다.)  

API 서버 포드의 정보를 YAML 포맷으로 출력해보면 YAML 파일에서 실제로 어떻게 설정돼 있는지 알 수 있다.  
``kubectl get pod kube-apiserver-k8s-m -n kube-system -o yaml | grep -F2 toleration``  
```  
->  
  securityContext: {}  
  terminationGracePeriodSeconds: 30  
  tolerations:  
  - effect: NoExecute  
    operator: Exists  		#
```  
operator 값은 Equal 외에도 Exists를 사용할 수 있다.  
Exists로 설정된 경우에는 Taint에 대한 와일드카드로서 동작하는데, key, value 및 effect 항목을 명시하지 않았다면 해당 항목의 값에 상관없이 모두 용인할 수 있다.  
(위 경우에는 key, value를 명시하지 않았기 때문에 모든 NoExecute 종류의 Taint에 대해 적용된다.)  

operator: Exists를 사용하는 예시  
1. 모든 종류의 Taint를 용인  
```  
  tolerations:  
    operator: Exists  
```  

2. 키의 이름이 my-key인 모든 Taint에 대해 값에 상관없이 용인  
```  
  tolerations:  
  -key: my-key  
   operator: Exists  
```  

3. 키의 이름이 my-key이고, Taint 효과가 NoExecute인 Taint에 대해 값에 상관없이 용인  
```  
  tolerations:  
  -key: my-key  
   effect: NoExecute  
   operator: Exists  
```  


#### NoExecute와 tolerationSeconds  
NoExecute는 포드를 해당 노드에 스케줄링하지 않을 뿐만 아니라 해당 노드에서 아예 포드를 실행할 수 없도록 설정한다.  
(NoSchedule은 노드에 설정하더라도 기존에 실행 중이던 포드는 정상적으로 동작하는 반면에,  
 NoExecute는 해당 노드에서 실행 중인 포드를 종료시킨다.)  
``kubectl taint node k8s-m alicek106/your-taint=your-taint-value:NoExecute``  
-> node/k8s-m tainted  

단, 포드가 디플로이먼트나 레플리카셋 등과 같은 리소스로부터 생성됐다면 NoExecute에 의해 포드가 종료됐더라도 포드는 다른 노드로 옮겨가는 포드의 퇴거(Eviction)가 발생한다.  
NoExecute에 의해 포드가 종료되면 레플리카셋은 라벨 셀렉터가 일치하는 포드의 개수가 replicas에 지정된 값보다 적다는 것을 감지할 것이고,  
다른 노드에 새롭게 포드를 생성할 것이기 때문이다.  

포드를 생성하면 쿠버네티스는 자동으로 NoExecute에 대한 Toleration을 추가한다.  
``kubectl describe pod deployment-recreate-688849b5cf-4czj7``  
```  
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s  
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s  
```  
(node.kubernetes.io/not-ready와 unreachable이라는 Taint에 대해 Toleration이 설정돼 있는데,  
이는 노드가 준비되지 않았거나 네트워크 통신이 불가능한 상황일 때를 위한 Toleration 이다.)  
예를 들어, kubectl get nodes에서 STATUS가 NotReady로 표시되는 경우에도 노드에 자동으로 Taint가 추가된다.  
~for 300s와 같은 Toleraiton은 바로 이러한 상황을 위한 것인데, 노드에 장애가 발생해 not-ready나 unreachable 상태의 Taint가 발생하더라도  
300초 동안은 해당 Taint를 용인하겠다는 뜻이다.  
300초 이내에 노드가 정상 상태로 돌아와 Taint가 삭제되지 않으면, 포드는 다른 노드로 옮겨가게 된다.  
(즉, 노드에 장애가 생겨도 해당 노드에서 실행 중이던 포드가 즉시 다른 노드로 옮겨가는 것은 아니며, 기본적으로 300초 후에 옮겨가게 된다.)  

이러한 옵션을 tolerationSeconds라고 부르며, 포드가 실행 중인 노드에 Taint가 추가됐을 때 해당 Taint를 용인할 수 있는 최대 시간을 의미한다.  
tolerationSeconds는 다음과 같은 형식으로 사용할 수 있다.  
``kubectl get pod <포드 이름> -o yaml | grep -F4 tolerationSeconds``  
``kubectl get pod deployment-recreate-688849b5cf-4czj7 -o yaml | grep -F4 tolerationSeconds``  
```  
  tolerations:  
  - effect: NoExecute  
    key: node.kubernetes.io/not-ready  
    operator: Exists  
    tolerationSeconds: 300  			#
  - effect: NoExecute  
    key: node.kubernetes.io/unreachable  
    operator: Exists  
    tolerationSeconds: 300  
  volumes:  
  - name: kube-api-access-7pm9d  
    projected:  
      defaultMode: 420  
```  


### 11.2.5 Cordon, Drain 및 PodDistributionBudget  
#### cordon을 이용한 스케줄링 비활성화  
쿠버네티스에서 제공하는 좀 더 명시적인 방법인 kubectl cordon 명령어를 사용하면 해당 노드에 더 이상 포드가 스케줄링되지 않는다.  
``kubectl cordon <노드 이름>``  
``kubectl cordon k8s-m``  
-> node/k8s-m cordoned  

``cordon을 해제하려면 uncordon 명령어를 사용한다.``  
``kubectl uncordon k8s-m``  
-> node/k8s-m uncordoned  
cordon 명령어로 지정된 노드는 새로운 포드가 할당되지 않는다.  
``kubectl get nodes`` 명령어로 노드의 상태를 확인해 보면 STATUS 항목에 SchedulingDisabled가 추가된 것읗 확인할 수 있다.  
```  
-> 
NAME     STATUS                     ROLES                  AGE   VERSION  
k8s-m    Ready,SchedulingDisabled   control-plane,master   27d   v1.21.4  
```  

``kubectl describe`` 명령어로 해당 노드에 어떠한 설정이 추가됐는지 확인할 수 있다.  
``kubectl describe node k8s-m``  
```  
->  
Taints:             alicek106/yout-taint=your-taint-value:NoExecute  
                    node-role.kubernetes.io/master:NoSchedule  
                    node.kubernetes.io/unschedulable:NoSchedule  
Unschedulable:      true  
```  
node.kubernetes.io/unschedulable:NoSchedule라는 이름의 Taint가 추가됐을 뿐만 아니라 Unschedulable 항목 또한 true로 설정돼 있다.  
단, 노드에 cordon 명령어를 사용하더라도 해당 노드에서 이미 실행 중인 포드가 종료되지는 않는다.  
cordon 명령어는 NoExecute가 아닌 NoSchedule 효과가 있는 Taint를 노드에 추가하기 때문이다.  
  

#### drain 명령어로 노드 비활성하기  
drain은 cordon처럼 해당 노드에 스케줄링을 금지한다는 것은 같지만, 노드에서 기존에 실행 중이던 포드를 다른 노드로 옮겨가도록 퇴거(Eviction)를 수행한다는 점이 다르다.  
drain 명령어를 사용하면 해당 노드에는 포드가 실행되지 않기 때문에 커널 버전 업그레이드, 유지 보수 등의 이유로 인해 잠시 노드를 중지해야 할 때 유용하게 사용할 수 있는 명령어이다.  
``kubectl drain <노드 이름>``  
``kubectl drain k8s-m``  
``` 
->   
node/k8s-m already cordoned  
error: unable to drain node "k8s-m", aborting command...  

There are pending nodes to be drained:  
 k8s-m  
error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-l4t9r,   kube-system/kube-proxy-4whdb  
```  
-> 포드가 스케줄링되지 않게 cordon은 설정됐지만, 해당 노드에서 실행 중이던 데몬셋 모드가 존재하기 때문에 drain을 할 수 없다는 에러가 출력된다.  
데몬셋을 무시하고 노드를 drain하려면 --ignore-daemonsets 옵션을 함께 사용한다.   

``kubectl drain k8s-m --ignore-daemonsets`` 
-> node/k8s-m drained  

디플로이먼트나 레플리카셋, 잡, 스테이트풀셋 등에 의해 생성되지 않은 포드, 즉 단일 포드가 노드에 존재할 때도 drain 명령어는 실패한다.  
(잡, 스테이트풀셋은 디플로이먼트처럼 포드를 사용하는 쿠버네티스 오브젝트이다.)  
이는 YAML 파일에서 type:Pod처럼 정의해 생성한 단일 포드는 어떠한 이유로 종료되더라도 다른 노드로 옮겨가 다시 생성되지 않기 때문이다.  
만약 단일 포드를 무시하고 노드를 drain하려면 --force 옵션을 함께 사용한다.  


drain과 같이 포드 퇴거 작업이 수행될 때는 한 가지 문제점이 있는데, 만약 drain된 노드에서 실행 중인 포드가 종료되어 해당 포드가 다른 노드로 옮겨가는 사이에는 애플리케이션이 중단될 수 있다.  
(kubectl drain을 사용했을 때, 새로운 포드가 다른 노드에서 다시 생성됨.)  


#### PodDisruptionBudget으로 포드 개수 유지하기  
PodDisruptionBudget은 kubectl drain 명령어 등으로 인해 포드에 퇴거가 발생할 때, 특정 개수 또는 비율만큼의 포드는 반드시 정상적인 상태를 유지하기 위해 사용된다.  
``kubectl get pdb``  
-> No resources found in default namespace  

``simple-pdb-example.yaml``  
```  
apiVersion: policy/v1beta1  
kind: PodDisruptionBudget  
metadata:  
  name: simple-pdb-example  
spec:  
maxUnavailable: 1        # 비활성화될 수 있는 포드의 최대 갯수 또는 비율 (%)  
  # minAvailable: 2  
  selector:                 # PDB의 대상이 될 포드를 선택하는 라벨 셀렉터  
    matchLabels:  
      app: webserver  
```  
maxUnavailable와 minAvailable 두 가지 중 하나를 사용할 수 있다.  
maxUnavailable은 kubectl drain 등에 의해 노드의 포드가 종료될 때 최대 몇 개까지 동시에 종료될 수 있는지를 뜻한다.  
위 예시처럼 1로 설정하면, kubectl drain 명령어를 사용한 노드의 포드가 하나씩 종료되어 다른 노드에서 다시 생성될 것이다.  
이 값은 숫자로 나타낼 수도 있지만, 전체 포드의 비율로 설정할 수도 있다.  

minAvailable은 포드의 퇴거가 발생할 때, 최소 몇 개의 포드가 정상 상태를 유지해야 하는지를 의미한다.  
의미만 다를 뿐 맥랑상으로는 같은 기능이므로 둘 중 하나만을 정의해야 한다.

selector에서는 적용될 포드의 라벨을 입력한다.  

포드를 삭제한다는것과 퇴거시키는 것은 결과론적으로 포드를 삭제한다는 점에서는 같지만 다른 개념이다.  
삭제는 PodDisruptionBudget과 상관없이 포드를 삭제하게 된다.  
퇴거는 쿠버네티스 내부에서 Evict라는 별도의 API로 구현돼 있으며, PodDisruptionBudget의 영향을 받아 포드를 삭제하게 된다.  



### 11.2.6 커스텀 스케줄러 및 스케줄러 확장 
#### 커스텀 스케줄러 구현 
포드를 생성하면 기본적으로 기본 스케줄러(kube-scheduler)를 사용하게 되며, 노드 필터링과 노드 스코어링 단계를 거쳐 스케줄링 작업을 수행한다. 
포드를 생성한 뒤에 자동으로 추가되는 schedulerName 항목에서 기본 스케줄러의 이름을 확인할 수 있다. 
``kubectl get pod debug -o yaml | grep scheduler`` 
-> schedulerName: default-scheduler 
기본 스케줄러는 포드의 schedulerName 값이 default-scheduler일 때만 해당 포드를 스케줄링한다. 
따라서 기본 스케줄러가 아닌 개인의 스케줄러로 포드를 스케줄링하고 싶다면, 포드를 생성할 때 별도로 명시하면 된다.  
```
apiVersion: v1 
kind: Pod 
metadata: 
  name: custom-scheduled-pod 
spec: 
  schedulerName: my-custom-scheduler 
  containers: 
  - name: nginx-container 
    image: nginx 
````
다음으로 해야할 일인 스케줄링할 수 있는 커스텀 스케줄러를 구현하는 단계(차례대로 소스코드로 구현한다.)  
1. API서버의 Watch API를 통해 새롭게 생성된 포드의 데이터를 받아온다.  
2. 포드의 데이터 중에서 nodeName이 설정돼 있지 않으며 schedulerName이 스케줄러의 정해진 이름과 일치하는지 검사한다.  
일반적으로 schedulerName은 스케줄러의 이름을 나타내는 고유한 값을 사용한다.  
3. 필요에 따라 적절한 스케줄링 알고리즘을 수행한 뒤, 바인딩 API 요청을 통해 스케줄링 된 노드의 이름을 포드의 nodeName에 설정한다.  
스케줄러를 구현하려면 API 서버와 통신해야 하기 때문에 스케줄러는 kube-scheduler 포드처럼 쿠버네티스 클러스터 내부에서 포드로 실행시키는 것이 일반적이다.  

#### 쿠버네티스 스케줄러 확장하기 
직접 커스텀 스케줄러를 구현하기보다는, 쿠버네티스 소스코드를 내려받아 스케줄러 부분만을 수정해 직접 빌드하는 것도 하나의 방법이 될 수 있다. 
또는 쿠버네티스 스케줄링 프레임워크를 사용해 스케줄러를 개발하거나, 스케줄러 Extender 등을 이용해 기본 스케줄러에 추가적인 로직을 덧붙이는 것도 가능하다. 
생각해보기: 라이브러리 VS 직접구현한 모듈 



### 11.3 쿠버네티스 애플리케이션 상태와 배포 
쿠버네티스에서 애플리케이션을 배포하기 위한 가장 쉬운 방법은 kubectl apply -f 명령어로 디플로이먼트를 생성함으로써 여러 개의 포드를 배포하는 방식이 있다. 
좀 더 고도화된 배포 방식을 원한다면 Spinnaker, Helm, Kustomize 또는 ArgoCD나 Jenkins와 같은 지속적 배포 도구(Continuous Delivery)를 사용할 수도 있다. 

애플리케이션이 반드시 무중단 상태를 유지해야 한다면 기존의 디플로이먼트를 삭제한 다음 새로운 디플로이먼트를 생성하는 작업으로 충분하지 않다.  
새롭게 생성하는 사이에 다운 타임이 발생할 수 있고, 삭제되는 포드가 처리 중이던 사용자 요청이 정상적으로 완료되지 않은 상태로 종료될 수도 있기 때문이다.  
쿠버네티스는 애플리케이션을 안정적으로 배포할 수 있도록 몇 가지 기능을 제공하고 있다. 
대표적으로는 새로운 버전의 애플리케이션이 점진적으로 배포될 수 있도록 디플로이먼트에서 롤링 업데이트 기능을 사용할 수 있으며, 
배포된 애플리케이션의 버전을 내부적으로 저장함으로써 언제든지 원하는 버전의 디플로이먼트로 되돌릴 수 있다. 
그뿐만 아니라 새롭게 배포되는 포드의 애플리케이션이 사용자의 요청을 처리할 준비가 됐는지 확인할 수 있고, 
삭제될 포드가 애플리케이션을 우아하게 종료될 수 있도록(Graceful Termination) 별도의 설정을 추가할 수도 있다. 


### 11.3.1 디플로이먼트를 통해 롤링 업데이트 
#### 디플로이먼트를 이용한 레플리카셋의 버전 관리  
테스트 또는 개발 환경이 아닌 한 포드를 직접 생성하는 경우는 거의 없다.  
대부분은 디플로이먼트를 생성하고, 디플로이먼트에 속하는 레플리카셋이 포드를 생성하는 것이 일반적이다.  
포드를 생성할 때 레플리카셋 대신 디플로이먼트를 사용하는 이유는,  
레플리카셋의 변경 사항을 저장하는 리비전을 디플로이먼트에서 관리함으로써,  
애플리케이션의 배포를 쉽게 하는 것이 목적이다.  
디플로이먼트에서 변경 사항이 생기면 새로운 레플리카셋이 생성되고, 그에 따라 새로운 버전의 애플리케이션이 배포된다. 
이때 --record 옵션을 추가해 디플로이먼트의 변경 사항을 적용하면, 이전에 사용하던 레플리카셋의 정보는 디플로이먼트의 히스토리에 기록된다. 
그리고 이러한 리비전을 이용해 언제든지 원하는 버전의 애플리케이션(레플리카셋)으로 롤백할 수 있다. 
``kubectl apply -f deployment-v1.yaml --record``  
-> deployment.apps/nginx-deployment created  
``kubectl get pods`` 
``` 
->
nginx-deployment-557f5d8bdb-bjnb6   0/1     Pending   0          31s 
nginx-deployment-557f5d8bdb-g75qj   0/1     Pending   0          31s 
nginx-deployment-557f5d8bdb-wn6ds   0/1     Pending   0          31s 
```  
``kubectl apply -f deployment-v2.yaml --record``  
-> deployment.apps/nginx-deployment configured 

``kubectl rollout history deployment nginx-deployment``  
```  
->  
deployment.apps/nginx-deployment   
REVISION  CHANGE-CAUSE   
1         kubectl apply --filename=deployment-v1.yaml --record=true   
2         kubectl apply --filename=deployment-v2.yaml --record=true   
```  
(기본적으로 레플리카셋의 리비전은 10개까지만 히스토리에 저장되지만, 
필요하다면 디플로이먼트를 생성할 때 revisionHistoryLimit이라는 항목을 직접 설정함으로써 리비전의 최대 개수를 지정할 수 있다.)  
``deployment-history-limit.yaml`` 
``` 
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: deployment-history-limit 
spec: 
  revisionHistoryLimit: 3 		# 
  replicas: 3 
  selector: 
    matchLabels: 
      app: nginx 
  template: 
    metadata: 
      name: nginx 
      labels: 
        app: nginx  
    spec:  
      containers:  
      - name: nginx  
        image: nginx:1.17  
        ports:  
        - containerPort: 80  
```  

#### 디플로이먼트를 통한 롤링 업데이트 설정  
디플로이먼트를 통해 새로운 버전의 포드를 생성하는 작업 그 자체는 매우 단순한 일이지만,  
배포 중에 애플리케이션이 중단돼도 괜찮은지에 따라 어떠한 배포 방법을 사용할 것인지 생각해 볼 필요가 있다.  
일시적으로 사용자의 요청을 처리하지 못해도 괜찮은 애플리케이션이라면 쿠버네티스에서 제공하는 ReCreate 방법을 사용할 수 있다.  
이 방법은 기존 버전의 포드를 모두 삭제한 뒤, 새로운 버전의 포드를 생성하는 방식이다.  
이러한 배포 전략은 디플로이먼트의 YAML 파일에 있는 stragy의 type 항목에서 설정할 수 있다.  
``deployment-recreate-v1.yaml`` 
``` 
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: deployment-recreate 
spec: 
  replicas: 3 
  strategy: 
    type: Recreate  	
  selector: 
    matchLabels: 
      app: nginx 
  template: 
    metadata: 
      name: nginx 
      labels: 
        app: nginx 
    spec: 
      containers: 
      - name: nginx 
        image: nginx:1.15 
        ports: 
        - containerPort: 80  
```  
``kubectl apply -f deployment-recreate-v1.yaml``  
-> deployment.apps/deployment-recreate created  

``kubectl get pods`` 
``` 
-> 
deployment-recreate-557f5d8bdb-4lpgb   0/1     Pending   0          24s 
deployment-recreate-557f5d8bdb-s4mmg   0/1     Pending   0          24s 
deployment-recreate-557f5d8bdb-vbzmg   0/1     Pending   0          24s 
``` 

``kubectl apply -f deployment-recreate-v2.yaml``  
-> deployment.apps/deployment-recreate configured  
쿠버네티스에서는 포드를 조금씩 삭제하고 생성하는 롤링 업데이트 기능을 제공한다. 
롤링 업데이트를 사용하면 디플로이먼틀르 업데이트하는 도중에도 사용자의 요청을 처리할 수 있는 포드가 계속 존재하기 때문에 애플리케이션의 중단이 발생하지 않는다. 
YAML 파일에서 별도의 설정을 하지 않아도 디플로이먼트의 버전을 업데이트할 때는 기본적으로 롤링 업데이트를 사용하도록 설정돼 있다. 
이 때, 롤링 업데이트 도중에 기존 버전의 포드를 몇 개씩 삭제할 것인지, 새로운 버전의 포드는 몇 개씩 생성할 것인지는 직접 설정할 수 있다. 
이러한 세부 옵션을 설정하려면 디플로이먼트를 정의하는 YAML 파일에서 명시적으로 strategy의 type 항목을 RollingUpdate로 설정해야 한다.  
``deployment-rolling-update.yaml`` 
``` 
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: deployment-rolling-update 
spec: 
  replicas: 3 
  strategy: 
    type: RollingUpdate		# 
    rollingUpdate: 
      maxSurge: 2  			# 
      maxUnavailable: 2  	# 
  selector: 
    matchLabels: 
      app: nginx 
  template: 
    metadata: 
      name: nginx 
      labels: 
        app: nginx 
    spec: 
      containers: 
      - name: nginx 
        image: nginx:1.15 
        ports: 
        - containerPort: 80 
``` 
롤링 업데이트의 세부 옵션으로 속도를 조절할 수 있다. 숫자나 비율을 값으로 사용할 수 있으며, 비율을 사용하면 전체 포드의 개수(디플로이먼트에 정의된 replicas 값)를 기준을 값이 결정된다. 
퍼센트를 사용할 때 maxSurge의 소수점 값은 반올림되고, maxUnavailable의 소수점 값은 버려진다. 
두 옵션 모두 기본값은 25%이다. 
- maxUnavailable: 롤링 업데이트 도중 사용 불가능한 상태가 되는 포드의 최대 개수를 설정한다. 
즉, 롤링 업데이트 도중에도 사용자의 요청이 처리될 수 있도록 실행 중인 포드의 개수가 일정 값 이하로 내려가지 않도록 유지한다. 
예를 들어 기본값이라면, 롤링 업데이트 도중 적어도 75%만큼의 포드는 사용자의 요청을 처리할 수 있는 상태로 유지한다.  
- maxSurge: 롤링 업데이트 도중 전체 포드의 개수가 디플로이먼트의 replicas 값보다 얼마나 더 많이 존재할 수 있는지 설정한다.  
이는 곧 새로운 버전의 포드를 얼마나 많이 생성될 수 있는지를 의미한다.  

예를 들어 기본값이라면, 이전버전의 포드 + 새로운 버전의 포드의 개수는 replicas 값 대비 최대 125%까지 늘어날 수 있다.  
만약 maxUnavailable의 값을 0으로 설정하면 롤링 업데이트 도중 전체 포드 개수는 적어도 replicas의 개수 만큼을 유지하게 된다. 
이때 maxSurge의 값도 0으로 함께 설정해 버리면 전체 포드 개수의 상한선이 하한선과 같아져 버리기 때문에 새로운 버전의 포드가 생성될 수 없으며, 롤링 업데이트가 진행되지 않는다. 
따라서 모두 0으로 설정하는 것은 허용되지 않는다. 
(롤링 업데이트를 사용하면 특정 순간에는 기존 버전과 새로운 버전의 애플리케이션이 공존할 수 있다. 
따라서 애플리케이션과 통신하는 다른 컴포넌트들은 기존 버전과 새로운 버전 중 어떠한 버전과 통신해도 전체 시스템에 문제가 발생하지 않아야 한다.) 


#### 블루 그린 배포 사용하기  
블루 그린 배포는 기존 버전의 포드를 그대로 놔둔 상태에서 새로운 버전의 포드를 미리 생성해 둔 뒤 서비스의 라우팅만 변경하는 배포 방식을 의미한다.  
블루 그린 배포는 롤링 업데이트와 달리 특정 순간에 두 버전의 애플리케이션이 공존하지 않으며, ReCreate 전략처럼 중단 시간이 발생하지도 않는다는 장점이 있다.  
(쿠버네티스가 자체적으로 지원하는 것은 아니다.)  
블루 그린 배포의 진행 순서 
1. 기존 버전(v1)의 디플로이먼트가 생성돼 있으며, 서비스는 사용자 요청을 v1 포드로 전달하고 있다. 
2. 새로운 버전(v2)의 디플로이먼트를 생성한다. 
3. 서비스의 라벨을 변경함으로써 서비스를 통한 요청이 새로운 버전의 디플로이먼트로 전달되도록 수정한다. 
4. 새로운 버전의 디플로이먼트가 잘 동작하는 것을 확인했다면, 기존 버전의 디플로이먼트를 삭제한다. 
만약 이전 버전으로의 롤백이 필요하다면 서비스의 자벨을 다시 아전 상태도 되돌린다.  
블루 그린 배포를 사용하면 특정 순간에는 디플로이먼트에 설정된 replica 개수의 두 배에 해당하는 포드가 실행되기 때문에 일시적으로 전체 자원을 많이 소모할 수 있다.



### 참고자료 
1.[파란하늘의 지식창고](https://luvstudy.tistory.com/106)  